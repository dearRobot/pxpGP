Job started on: g005
SLURM_JOB_ID: 47702044
Running pxpGP 1 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 59.91 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 1 completed successfully
Running pxpGP 2 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 59.68 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 2 completed successfully
Running pxpGP 3 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.22 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 3 completed successfully
Running pxpGP 4 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.06 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 4 completed successfully
Running pxpGP 5 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.50 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 5 completed successfully
Running pxpGP 6 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.84 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 6 completed successfully
Running pxpGP 7 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.89 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 7 completed successfully
Running pxpGP 8 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 61.00 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 8 completed successfully
Running pxpGP 9 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.32 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 9 completed successfully
Running pxpGP 10 with agents: 64
[92mRank 0 - sparse dataset size is: 4, local dataset: torch.Size([289, 2]), [0m
[92mRank 0 - Training local sparse GP model with 289 samples[0m
Epoch 1/200 - Loss: 3.774
Epoch 11/200 - Loss: 1.035
Epoch 21/200 - Loss: 0.816
Epoch 31/200 - Loss: 0.431
Epoch 41/200 - Loss: 0.018
Converged at epoch 42 with loss -0.076
[92mRank 0 - Lengthscale: [[0.71645516 0.51002926]] [0m
[92mRank 0 - Outputscale: 1.96975839138031 [0m
[92mRank 0 - Noise: 0.09755183756351471 [0m
Rank 0 - Augmented dataset size: 545
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.71645516 0.51002926]]
Rank: 0, Outputscale: 1.96975839138031
Rank: 0, Noise: 0.09755183756351471
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.5482262372970581, rho: 1.0000, lip: 0.6821
rank 0, epoch 19, loss: 0.5351511836051941, rho: 0.5000, lip: 0.3421
rank 0, epoch 29, loss: 0.5307295918464661, rho: 0.5000, lip: 0.1538
rank 0, epoch 39, loss: 0.5289841890335083, rho: 0.5000, lip: 0.0817
rank 0, epoch 49, loss: 0.5282196998596191, rho: 0.5000, lip: 0.0504
rank 0, epoch 59, loss: 0.5275034308433533, rho: 0.5000, lip: 0.0331
rank 0, epoch 69, loss: 0.5247927904129028, rho: 0.5000, lip: 0.0380
rank 0, epoch 79, loss: 0.5205467939376831, rho: 0.5000, lip: 0.0860
rank 0, epoch 89, loss: 0.5157773494720459, rho: 0.5000, lip: 0.1628
rank 0, epoch 99, loss: 0.5171027183532715, rho: 0.5000, lip: 0.1128
rank 0, epoch 109, loss: 0.5182546973228455, rho: 0.5000, lip: 0.0885
rank 0, epoch 119, loss: 0.5190470814704895, rho: 0.5000, lip: 0.0655
scaled pxADMM converged at iteration 127
[92mpxpGP Converged at epoch 127, with loss 0.5100, rho: 0.5000, lip: 0.0519[0m
Rank 0 - Training time: 60.41 seconds
[92mRank 0 - Testing RMSE: 0.3958[0m
[92mRank: 0, Lengthscale: [[0.6049906 0.3221283]] [0m
[92mRank: 0, Outputscale: 1.7547756433486938 [0m
[92mRank: 0, Noise: 0.1036597415804863 [0m
Run 10 completed successfully
Running gapxGP 1 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.93 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 1 completed successfully
Running gapxGP 2 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.23 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 2 completed successfully
Running gapxGP 3 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.04 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 3 completed successfully
Running gapxGP 4 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.66 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 4 completed successfully
Running gapxGP 5 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.04 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 5 completed successfully
Running gapxGP 6 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.85 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 6 completed successfully
Running gapxGP 7 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.88 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 7 completed successfully
Running gapxGP 8 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.63 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 8 completed successfully
Running gapxGP 9 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.94 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 9 completed successfully
Running gapxGP 10 with agents: 64
Rank 0 - Augmented dataset size: 541
Rank 0 - Local dataset size: 289
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.74 seconds
[92mRank 0 - Testing RMSE: 0.1574[0m
[92mRank: 0, Lengthscale: [[0.67217267 0.44239977]] [0m
[92mRank: 0, Outputscale: 1.2084741592407227 [0m
[92mRank: 0, Noise: 0.08089247345924377 [0m
Run 10 completed successfully
Running apxGP 1 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 3.45 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 1 completed successfully
Running apxGP 2 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 4.50 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 2 completed successfully
Running apxGP 3 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.51 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 3 completed successfully
Running apxGP 4 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 4.16 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 4 completed successfully
Running apxGP 5 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.98 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 5 completed successfully
Running apxGP 6 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.49 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 6 completed successfully
Running apxGP 7 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.76 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 7 completed successfully
Running apxGP 8 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.25 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 8 completed successfully
Running apxGP 9 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 2.49 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 9 completed successfully
Running apxGP 10 with agents: 64
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.3318539559841156
Rank 0: Training completed in 4.11 seconds.
[92mRank 0 - Testing RMSE: 3.2512[0m
[92mRank: 0, Lengthscale: [[0.6752857 0.5961432]] [0m
[92mRank: 0, Outputscale: 0.8450008034706116 [0m
[92mRank: 0, Noise: 0.06173080578446388 [0m
Run 10 completed successfully
Running cGP 1 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145

Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
[92mRank 0 - Testing RMSE: 5.4577[0m
[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
[92mRank: 0, Noise: 0.019780568778514862 [0m
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342

Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Run 1 completed successfully
Running cGP 2 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003

Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925


Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145

Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898

Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
[92mRank 0 - Testing RMSE: 5.4577[0m
[92mRank: 0, Lengthscale: Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
[[0.139583   0.13595665]] [0m
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308

Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838

Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
[92mRank: 0, Noise: 0.019780568778514862 [0m
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074

Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657

Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Run 2 completed successfully
Running cGP 3 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365

Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425

Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
[92mRank 0 - Testing RMSE: 5.4577[0m
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145

[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103[92mRank: 0, Noise:Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
 0.019780568778514862 [0m
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166

Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519

Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874

Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705

Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Run 3 completed successfully
Running cGP 4 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176


Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
[92mRank 0 - Testing RMSE: 5.4577[0m
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461

[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m

Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578


Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874

Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
[92mRank: 0, Noise: 0.019780568778514862 [0m
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585

Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469

Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Run 4 completed successfully
Running cGP 5 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925

Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419

Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868

Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594

Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583

Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
[92mRank 0 - Testing RMSE: 5.4577[0m
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
[92mRank: 0, Lengthscale: [[0.139583   0.13595665]]Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
 [0m
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
[92mRank: 0, Noise: 0.019780568778514862 [0m
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288

Run 5 completed successfully
Running cGP 6 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425

Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712


Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782

Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
[92mRank 0 - Testing RMSE: 5.4577[0m
[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
[92mRank: 0, Noise: 0.019780568778514862 [0m
Run 6 completed successfully
Running cGP 7 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831


Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705


Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416

Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965

Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868

Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
[92mRank 0 - Testing RMSE: 5.4577[0m
Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
[92mRank: 0, Noise: 0.019780568778514862 [0m
Run 7 completed successfully
Running cGP 8 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464

Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074

Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519

Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469

Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288

[92mRank 0 - Testing RMSE: 5.4577[0m
[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
[92mRank: 0, Noise: 0.019780568778514862 [0m
Run 8 completed successfully
Running cGP 9 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419
Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166

Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372


Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831

Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
[92mRank 0 - Testing RMSE: 5.4577[0m
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874

[92mRank: 0, Lengthscale: [[0.139583   0.13595665]] [0m
Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
[92mRank: 0, Noise:Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
 0.019780568778514862 [0m
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452
Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046

Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782

Run 9 completed successfully
Running cGP 10 with agents: 64
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.743550181388855
Epoch 20/1000 Loss: 0.631023645401001
Epoch 30/1000 Loss: 0.521934986114502
Epoch 40/1000 Loss: 0.4229099154472351
Epoch 50/1000 Loss: 0.34009799361228943
Epoch 60/1000 Loss: 0.2802453637123108
Epoch 70/1000 Loss: 0.25027787685394287
Epoch 80/1000 Loss: 0.2527709901332855
Epoch 90/1000 Loss: 0.27617770433425903
Epoch 100/1000 Loss: 0.29724401235580444
Epoch 110/1000 Loss: 0.296027809381485
Epoch 120/1000 Loss: 0.26322847604751587
Rank 0 - Training converged at epoch 126 with loss: 0.22686198353767395
Rank 55 - Training converged at epoch 126 with loss: 0.17858342826366425
Rank 61 - Training converged at epoch 126 with loss: 0.18820521235466003
Rank 63 - Training converged at epoch 126 with loss: -0.09523902833461761
Rank 52 - Training converged at epoch 126 with loss: -0.04767650365829468
Rank 50 - Training converged at epoch 126 with loss: 0.38377535343170166
Rank 60 - Training converged at epoch 126 with loss: 0.25889796018600464
Rank 56 - Training converged at epoch 126 with loss: 0.41339218616485596Rank 48 - Training converged at epoch 126 with loss: 0.5852397084236145
Rank 49 - Training converged at epoch 126 with loss: 0.6852240562438965
Rank 62 - Training converged at epoch 126 with loss: 0.10204870253801346
Rank 58 - Training converged at epoch 126 with loss: 0.12644729018211365

Rank 59 - Training converged at epoch 126 with loss: 0.1261022835969925
Rank 53 - Training converged at epoch 126 with loss: -0.1470780372619629
Rank 54 - Training converged at epoch 126 with loss: 0.16285264492034912
Rank 40 - Training converged at epoch 126 with loss: 0.6943244338035583
Rank 36 - Training converged at epoch 126 with loss: -0.3701246976852417Rank 38 - Training converged at epoch 126 with loss: 0.20267009735107422
Rank 47 - Training converged at epoch 126 with loss: 0.3101500868797302
Rank 32 - Training converged at epoch 126 with loss: 0.7663354873657227
Rank 57 - Training converged at epoch 126 with loss: 0.4962073564529419

Rank 31 - Training converged at epoch 126 with loss: 0.3729059100151062Rank 33 - Training converged at epoch 126 with loss: 0.7709602117538452

Rank 51 - Training converged at epoch 126 with loss: 0.026310086250305176
Rank 30 - Training converged at epoch 126 with loss: 0.27256453037261963Rank 42 - Training converged at epoch 126 with loss: 0.5485492944717407

Rank 34 - Training converged at epoch 126 with loss: 0.5976649522781372
Rank 28 - Training converged at epoch 126 with loss: -0.031183481216430664Rank 43 - Training converged at epoch 126 with loss: 0.1438826322555542Rank 41 - Training converged at epoch 126 with loss: 0.733407735824585
Rank 45 - Training converged at epoch 126 with loss: -0.27685701847076416

Rank 2 - Training converged at epoch 126 with loss: 0.29921022057533264
Rank 10 - Training converged at epoch 126 with loss: 0.3203986883163452

Rank 22 - Training converged at epoch 126 with loss: 0.45300185680389404
Rank 12 - Training converged at epoch 126 with loss: 0.6401044130325317
Rank 17 - Training converged at epoch 126 with loss: 0.6161031723022461
Rank 15 - Training converged at epoch 126 with loss: 0.5585059523582458
Rank 7 - Training converged at epoch 126 with loss: 0.6085286736488342
Rank 18 - Training converged at epoch 126 with loss: 0.31134045124053955
Rank 27 - Training converged at epoch 126 with loss: 0.03643465042114258
Rank 13 - Training converged at epoch 126 with loss: 0.7397494316101074
Rank 8 - Training converged at epoch 126 with loss: 0.42023923993110657
Rank 5 - Training converged at epoch 126 with loss: 0.7768990397453308
Rank 29 - Training converged at epoch 126 with loss: 0.21269237995147705
Rank 14 - Training converged at epoch 126 with loss: 0.6946525573730469
Rank 46 - Training converged at epoch 126 with loss: 0.2028716802597046
Rank 19 - Training converged at epoch 126 with loss: 0.08969557285308838
Rank 3 - Training converged at epoch 126 with loss: 0.46958693861961365
Rank 44 - Training converged at epoch 126 with loss: -0.3604058027267456
Rank 26 - Training converged at epoch 126 with loss: 0.48325276374816895
Rank 6 - Training converged at epoch 126 with loss: 0.7790891528129578
Rank 35 - Training converged at epoch 126 with loss: 0.18694829940795898
Rank 16 - Training converged at epoch 126 with loss: 0.5218895673751831
[92mRank 0 - Testing RMSE: 5.4577[0m
[92mRank: 0, Lengthscale: Rank 11 - Training converged at epoch 126 with loss: 0.4551318883895874
[[0.139583   0.13595665]] [0m
Rank 23 - Training converged at epoch 126 with loss: 0.36903026700019836
Rank 9 - Training converged at epoch 126 with loss: 0.5309908390045166
Rank 4 - Training converged at epoch 126 with loss: 0.6558378338813782
Rank 39 - Training converged at epoch 126 with loss: 0.3816211521625519
Rank 25 - Training converged at epoch 126 with loss: 0.7337913513183594
[92mRank: 0, Outputscale: 0.20733971893787384 [0m
Rank 24 - Training converged at epoch 126 with loss: 0.6871706247329712
Rank 20 - Training converged at epoch 126 with loss: 0.3630167245864868
Rank 37 - Training converged at epoch 126 with loss: -0.06941664218902588
[92mRank: 0, Noise: 0.019780568778514862 [0m
Rank 21 - Training converged at epoch 126 with loss: 0.5075842142105103
Rank 1 - Training converged at epoch 126 with loss: 0.3410296142101288
Run 10 completed successfully
Running pxpGP 1 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 41.00 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 1 completed successfully
Running pxpGP 2 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.99 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 2 completed successfully
Running pxpGP 3 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.81 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 3 completed successfully
Running pxpGP 4 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.80 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 4 completed successfully
Running pxpGP 5 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.74 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 5 completed successfully
Running pxpGP 6 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.92 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 6 completed successfully
Running pxpGP 7 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.56 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 7 completed successfully
Running pxpGP 8 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.80 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 8 completed successfully
Running pxpGP 9 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 40.86 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 9 completed successfully
Running pxpGP 10 with agents: 100
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([169, 2]), [0m
[92mRank 0 - Training local sparse GP model with 169 samples[0m
Epoch 1/200 - Loss: 2.416
Epoch 11/200 - Loss: 1.070
Epoch 21/200 - Loss: 0.775
Epoch 31/200 - Loss: 0.307
Converged at epoch 39 with loss -0.018
[92mRank 0 - Lengthscale: [[0.9902079 0.5418506]] [0m
[92mRank 0 - Outputscale: 1.650136947631836 [0m
[92mRank 0 - Noise: 0.09760354459285736 [0m
Rank 0 - Augmented dataset size: 369
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.9902079 0.5418506]]
Rank: 0, Outputscale: 1.650136947631836
Rank: 0, Noise: 0.09760355949401855
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.9294331669807434, rho: 1.0000, lip: 0.7339
rank 0, epoch 19, loss: 0.7766280174255371, rho: 0.5000, lip: 0.3482
rank 0, epoch 29, loss: 0.7530914545059204, rho: 0.5000, lip: 0.1938
rank 0, epoch 39, loss: 0.745224118232727, rho: 0.5000, lip: 0.0893
rank 0, epoch 49, loss: 0.7413267493247986, rho: 0.5000, lip: 0.0540
rank 0, epoch 59, loss: 0.7390196323394775, rho: 0.5000, lip: 0.0395
rank 0, epoch 69, loss: 0.7375952005386353, rho: 0.5000, lip: 0.0428
rank 0, epoch 79, loss: 0.7367858290672302, rho: 0.5000, lip: 0.0540
rank 0, epoch 89, loss: 0.7363618612289429, rho: 0.5000, lip: 0.0630
rank 0, epoch 99, loss: 0.7362035512924194, rho: 0.5000, lip: 0.0408
scaled pxADMM converged at iteration 106
[92mpxpGP Converged at epoch 106, with loss 0.7359, rho: 0.5000, lip: 0.0395[0m
Rank 0 - Training time: 41.00 seconds
[92mRank 0 - Testing RMSE: 0.3183[0m
[92mRank: 0, Lengthscale: [[0.5982723 0.3440436]] [0m
[92mRank: 0, Outputscale: 2.0215630531311035 [0m
[92mRank: 0, Noise: 0.13444852828979492 [0m
Run 10 completed successfully
Running gapxGP 1 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.22 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 1 completed successfully
Running gapxGP 2 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.51 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 2 completed successfully
Running gapxGP 3 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.16 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 3 completed successfully
Running gapxGP 4 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.69 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 4 completed successfully
Running gapxGP 5 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.16 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 5 completed successfully
Running gapxGP 6 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.23 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 6 completed successfully
Running gapxGP 7 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.42 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 7 completed successfully
Running gapxGP 8 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.76 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 8 completed successfully
Running gapxGP 9 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 5.02 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 9 completed successfully
Running gapxGP 10 with agents: 100
Rank 0 - Augmented dataset size: 367
Rank 0 - Local dataset size: 169
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.55 seconds
[92mRank 0 - Testing RMSE: 0.2226[0m
[92mRank: 0, Lengthscale: [[0.69805723 0.4812392 ]] [0m
[92mRank: 0, Outputscale: 1.2926779985427856 [0m
[92mRank: 0, Noise: 0.10375956445932388 [0m
Run 10 completed successfully
Running apxGP 1 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 3.69 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 1 completed successfully
Running apxGP 2 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 3.69 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 2 completed successfully
Running apxGP 3 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 2.10 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 3 completed successfully
Running apxGP 4 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 2.28 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 4 completed successfully
Running apxGP 5 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 4.21 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 5 completed successfully
Running apxGP 6 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 1.78 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 6 completed successfully
Running apxGP 7 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 2.01 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 7 completed successfully
Running apxGP 8 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 1.80 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 8 completed successfully
Running apxGP 9 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 5.90 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 9 completed successfully
Running apxGP 10 with agents: 100
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2845005691051483
Rank 0: Training completed in 2.29 seconds.
[92mRank 0 - Testing RMSE: 3.2133[0m
[92mRank: 0, Lengthscale: [[0.6766667 0.6085956]] [0m
[92mRank: 0, Outputscale: 0.8869953155517578 [0m
[92mRank: 0, Noise: 0.06288979202508926 [0m
Run 10 completed successfully
Running cGP 1 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754

Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105
Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634

Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356

Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739

Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261

Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248

Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447

Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716

Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194

Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063

Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947

Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036

Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343


Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824

Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842


[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
[[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Run 1 completed successfully
Running cGP 2 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231
Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468
Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113
Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187
Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928

Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544

Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006


Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255

Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343

Run 2 completed successfully
Running cGP 3 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468
Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187

Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113

Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341

Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476

Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964

Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634

Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447

Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231

Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716

Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667

Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716

Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049



Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Run 3 completed successfully
Running cGP 4 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873

Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468

Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608

Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443

Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105

Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356

Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842

Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849

Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803

Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784

Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049

Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Run 4 completed successfully
Running cGP 5 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468

Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113
Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754

Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248

Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544

Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276

Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887

Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483

Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343
Run 5 completed successfully
Running cGP 6 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231

Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207

Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749

Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187

Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852

Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779

Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447



Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261

Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727

Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975

Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276

Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862[92mRank 0 - Testing RMSE: 5.4673[0m

[92mRank: 0, Lengthscale: Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
[[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828

Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667

Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825

Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308

Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343


Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248

Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085

Run 6 completed successfully
Running cGP 7 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773
Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468


Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754

Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634

Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975

Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411

Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
[92mRank 0 - Testing RMSE: 5.4673[0m
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
[92mRank: 0, Lengthscale: Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
[[0.13172364 0.12262816]] [0m
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608[92mRank: 0, Outputscale: 0.22620537877082825
 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105

Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784

Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255


Run 7 completed successfully
Running cGP 8 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207


Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113

Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187
Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468
Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852

Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666

Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905

Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261

Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411

Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072

Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006

Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206

Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667

Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663

Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105

Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225

Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884

Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037

Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
[92mRank 0 - Testing RMSE: 5.4673[0mRank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104

[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
[0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Run 8 completed successfully
Running cGP 9 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105


Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468

Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187

Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749

Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295

Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873
Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443

Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849

Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405



Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716

Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663

Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663

Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
[92mRank 0 - Testing RMSE: 5.4673[0mRank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248

Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255

Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225

Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049

Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784

Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036

Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Run 9 completed successfully
Running cGP 10 with agents: 100
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7791528701782227
Epoch 20/1000 Loss: 0.6695497632026672
Epoch 30/1000 Loss: 0.5623118281364441
Epoch 40/1000 Loss: 0.4654228091239929
Epoch 50/1000 Loss: 0.38660895824432373
Epoch 60/1000 Loss: 0.3307880163192749
Epoch 70/1000 Loss: 0.3003246486186981
Epoch 80/1000 Loss: 0.2950286865234375
Epoch 90/1000 Loss: 0.30950847268104553
Epoch 100/1000 Loss: 0.3292071223258972
Epoch 110/1000 Loss: 0.33371469378471375
Epoch 120/1000 Loss: 0.31017065048217773
Epoch 130/1000 Loss: 0.25789692997932434
Rank 97 - Training converged at epoch 133 with loss: 0.2222307175397873Rank 98 - Training converged at epoch 133 with loss: 0.07773666828870773Rank 99 - Training converged at epoch 133 with loss: -0.1342194676399231

Rank 0 - Training converged at epoch 133 with loss: 0.23744715750217438
Rank 96 - Training converged at epoch 133 with loss: 0.31052204966545105Rank 95 - Training converged at epoch 133 with loss: 0.3576204180717468
Rank 94 - Training converged at epoch 133 with loss: 0.22166885435581207


Rank 92 - Training converged at epoch 133 with loss: 0.36224979162216187Rank 90 - Training converged at epoch 133 with loss: 0.46956107020378113

Rank 86 - Training converged at epoch 133 with loss: 0.04481000825762749Rank 83 - Training converged at epoch 133 with loss: 0.2265954613685608

Rank 89 - Training converged at epoch 133 with loss: 0.13672471046447754
Rank 80 - Training converged at epoch 133 with loss: 0.624244213104248
Rank 76 - Training converged at epoch 133 with loss: -0.3558390140533447
Rank 77 - Training converged at epoch 133 with loss: -0.041834812611341476
Rank 74 - Training converged at epoch 133 with loss: 0.040593795478343964
Rank 69 - Training converged at epoch 133 with loss: 0.40995246171951294
Rank 87 - Training converged at epoch 133 with loss: 0.045438505709171295
Rank 63 - Training converged at epoch 133 with loss: 0.5637953877449036
Rank 79 - Training converged at epoch 133 with loss: 0.314843088388443
Rank 78 - Training converged at epoch 133 with loss: 0.3324629068374634
Rank 70 - Training converged at epoch 133 with loss: 0.7085574269294739
Rank 84 - Training converged at epoch 133 with loss: 0.045848868787288666
Rank 59 - Training converged at epoch 133 with loss: 0.47147589921951294
Rank 57 - Training converged at epoch 133 with loss: 0.12623921036720276
Rank 81 - Training converged at epoch 133 with loss: 0.7463385462760925
Rank 58 - Training converged at epoch 133 with loss: 0.3356786370277405
Rank 60 - Training converged at epoch 133 with loss: 0.816703200340271
Rank 56 - Training converged at epoch 133 with loss: -0.12018779665231705
Rank 55 - Training converged at epoch 133 with loss: -0.4124089479446411
Rank 54 - Training converged at epoch 133 with loss: 0.1669333130121231
Rank 52 - Training converged at epoch 133 with loss: 0.7531818747520447
Rank 68 - Training converged at epoch 133 with loss: 0.3504253923892975
Rank 88 - Training converged at epoch 133 with loss: 0.22839228808879852
Rank 82 - Training converged at epoch 133 with loss: 0.5764303803443909
Rank 62 - Training converged at epoch 133 with loss: 0.7272375822067261
Rank 66 - Training converged at epoch 133 with loss: -0.3566807806491852
Rank 49 - Training converged at epoch 133 with loss: 0.460174024105072Rank 65 - Training converged at epoch 133 with loss: -0.4103790819644928
Rank 72 - Training converged at epoch 133 with loss: 0.6732725501060486Rank 44 - Training converged at epoch 133 with loss: 0.01416196208447218
Rank 41 - Training converged at epoch 133 with loss: 0.8820933699607849
Rank 42 - Training converged at epoch 133 with loss: 0.7151558995246887
Rank 39 - Training converged at epoch 133 with loss: 0.4111637771129608

Rank 51 - Training converged at epoch 133 with loss: 0.9221208691596985
Rank 73 - Training converged at epoch 133 with loss: 0.43085139989852905
Rank 36 - Training converged at epoch 133 with loss: 0.46080702543258667

Rank 64 - Training converged at epoch 133 with loss: 0.16136638820171356
Rank 61 - Training converged at epoch 133 with loss: 0.8716211318969727
Rank 53 - Training converged at epoch 133 with loss: 0.5642728209495544
Rank 45 - Training converged at epoch 133 with loss: -0.1168883740901947
Rank 27 - Training converged at epoch 133 with loss: 0.6655510663986206
Rank 85 - Training converged at epoch 133 with loss: 0.12598676979541779
Rank 47 - Training converged at epoch 133 with loss: 0.2718305289745331
Rank 28 - Training converged at epoch 133 with loss: 0.6149928569793701Rank 22 - Training converged at epoch 133 with loss: 0.5395408272743225
Rank 19 - Training converged at epoch 133 with loss: 0.6867647767066956
Rank 23 - Training converged at epoch 133 with loss: 0.21989721059799194
Rank 32 - Training converged at epoch 133 with loss: 0.62489914894104
Rank 24 - Training converged at epoch 133 with loss: 0.36827167868614197
Rank 26 - Training converged at epoch 133 with loss: 0.6996589303016663Rank 93 - Training converged at epoch 133 with loss: 0.062125932425260544
Rank 20 - Training converged at epoch 133 with loss: 0.46060270071029663
Rank 12 - Training converged at epoch 133 with loss: 0.4586969017982483
Rank 8 - Training converged at epoch 133 with loss: 0.8628596663475037
Rank 13 - Training converged at epoch 133 with loss: 0.39963337779045105

Rank 9 - Training converged at epoch 133 with loss: 0.7063897252082825
Rank 48 - Training converged at epoch 133 with loss: 0.3496929705142975Rank 31 - Training converged at epoch 133 with loss: 0.777199923992157

Rank 25 - Training converged at epoch 133 with loss: 0.5793772339820862
Rank 17 - Training converged at epoch 133 with loss: 0.8522357940673828
Rank 3 - Training converged at epoch 133 with loss: 0.3996583819389343
Rank 40 - Training converged at epoch 133 with loss: 0.7853524684906006
Rank 4 - Training converged at epoch 133 with loss: 0.5408031940460205
Rank 1 - Training converged at epoch 133 with loss: 0.3739575147628784
Rank 10 - Training converged at epoch 133 with loss: 0.40429553389549255
Rank 2 - Training converged at epoch 133 with loss: 0.3279740512371063
Rank 15 - Training converged at epoch 133 with loss: 0.7421613931655884
Rank 7 - Training converged at epoch 133 with loss: 0.9153681397438049
Rank 29 - Training converged at epoch 133 with loss: 0.5118198990821838
Rank 16 - Training converged at epoch 133 with loss: 0.8555163145065308
Rank 35 - Training converged at epoch 133 with loss: 0.25616464018821716
Rank 50 - Training converged at epoch 133 with loss: 0.8698241710662842
Rank 6 - Training converged at epoch 133 with loss: 0.8511247038841248
Rank 91 - Training converged at epoch 133 with loss: 0.5880328416824341

Rank 33 - Training converged at epoch 133 with loss: 0.21397778391838074
Rank 71 - Training converged at epoch 133 with loss: 0.8089057207107544
Rank 34 - Training converged at epoch 133 with loss: -0.016624314710497856
Rank 38 - Training converged at epoch 133 with loss: 0.4250381886959076
Rank 75 - Training converged at epoch 133 with loss: -0.1931028664112091
Rank 30 - Training converged at epoch 133 with loss: 0.5962326526641846
Rank 18 - Training converged at epoch 133 with loss: 0.8269267678260803
Rank 14 - Training converged at epoch 133 with loss: 0.5981825590133667
Rank 37 - Training converged at epoch 133 with loss: 0.47210702300071716
Rank 67 - Training converged at epoch 133 with loss: 0.03285253420472145
Rank 43 - Training converged at epoch 133 with loss: 0.42550188302993774
Rank 5 - Training converged at epoch 133 with loss: 0.7128674387931824
Rank 11 - Training converged at epoch 133 with loss: 0.569467306137085
Rank 21 - Training converged at epoch 133 with loss: 0.673854410648346
Rank 46 - Training converged at epoch 133 with loss: 0.16720499098300934
[92mRank 0 - Testing RMSE: 5.4673[0m
[92mRank: 0, Lengthscale: [[0.13172364 0.12262816]] [0m
[92mRank: 0, Outputscale: 0.22620537877082825 [0m
[92mRank: 0, Noise: 0.016776680946350098 [0m
Run 10 completed successfully
Running pxpGP 1 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.97 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 1 completed successfully
Running pxpGP 2 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 46.00 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 2 completed successfully
Running pxpGP 3 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.93 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 3 completed successfully
Running pxpGP 4 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.94 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 4 completed successfully
Running pxpGP 5 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.92 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 5 completed successfully
Running pxpGP 6 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.76 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 6 completed successfully
Running pxpGP 7 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.68 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 7 completed successfully
Running pxpGP 8 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 46.05 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 8 completed successfully
Running pxpGP 9 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 46.26 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 9 completed successfully
Running pxpGP 10 with agents: 121
[92mRank 0 - sparse dataset size is: 2, local dataset: torch.Size([144, 2]), [0m
[92mRank 0 - Training local sparse GP model with 144 samples[0m
Epoch 1/200 - Loss: 2.499
Epoch 11/200 - Loss: 0.925
Epoch 21/200 - Loss: 0.641
Epoch 31/200 - Loss: 0.743
Converged at epoch 36 with loss -0.026
[92mRank 0 - Lengthscale: [[0.98306966 0.46320146]] [0m
[92mRank 0 - Outputscale: 1.4049080610275269 [0m
[92mRank 0 - Noise: 0.0928952768445015 [0m
Rank 0 - Augmented dataset size: 386
Rank 0: After warm start model parameters:
Rank: 0, Lengthscale: [[0.98306966 0.46320146]]
Rank: 0, Outputscale: 1.4049080610275269
Rank: 0, Noise: 0.0928952768445015
[92mRank 0 - Training global model with pxADMM optimizer[0m
rank 0, epoch 9, loss: 0.8714851140975952, rho: 1.0000, lip: 0.7407
rank 0, epoch 19, loss: 0.7715933322906494, rho: 0.5000, lip: 0.3898
rank 0, epoch 29, loss: 0.7626187205314636, rho: 0.5000, lip: 0.1901
rank 0, epoch 39, loss: 0.7580596208572388, rho: 0.5000, lip: 0.0840
rank 0, epoch 49, loss: 0.7557032108306885, rho: 0.5000, lip: 0.0487
rank 0, epoch 59, loss: 0.7543931007385254, rho: 0.5000, lip: 0.0329
rank 0, epoch 69, loss: 0.7532886266708374, rho: 0.5000, lip: 0.0309
rank 0, epoch 79, loss: 0.7521449327468872, rho: 0.5000, lip: 0.0687
rank 0, epoch 89, loss: 0.7515730261802673, rho: 0.5000, lip: 0.0681
rank 0, epoch 99, loss: 0.7516795992851257, rho: 0.5000, lip: 0.0520
scaled pxADMM converged at iteration 101
[92mpxpGP Converged at epoch 101, with loss 0.7503, rho: 0.5000, lip: 0.0520[0m
Rank 0 - Training time: 45.92 seconds
[92mRank 0 - Testing RMSE: 0.3033[0m
[92mRank: 0, Lengthscale: [[0.70267946 0.43925852]] [0m
[92mRank: 0, Outputscale: 2.2464447021484375 [0m
[92mRank: 0, Noise: 0.1597435176372528 [0m
Run 10 completed successfully
Running gapxGP 1 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 7.46 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 1 completed successfully
Running gapxGP 2 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 3.94 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 2 completed successfully
Running gapxGP 3 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.14 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 3 completed successfully
Running gapxGP 4 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.53 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 4 completed successfully
Running gapxGP 5 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.20 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 5 completed successfully
Running gapxGP 6 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 7.85 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 6 completed successfully
Running gapxGP 7 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.24 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 7 completed successfully
Running gapxGP 8 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 4.06 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 8 completed successfully
Running gapxGP 9 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 6.99 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 9 completed successfully
Running gapxGP 10 with agents: 121
Rank 0 - Augmented dataset size: 384
Rank 0 - Local dataset size: 144
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 8
Converged at epoch 8
Rank 0 - Training time: 8.54 seconds
[92mRank 0 - Testing RMSE: 0.1983[0m
[92mRank: 0, Lengthscale: [[0.69174445 0.46455663]] [0m
[92mRank: 0, Outputscale: 1.302987813949585 [0m
[92mRank: 0, Noise: 0.10395003110170364 [0m
Run 10 completed successfully
Running apxGP 1 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 2.14 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 1 completed successfully
Running apxGP 2 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 2.92 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 2 completed successfully
Running apxGP 3 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 2.90 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 3 completed successfully
Running apxGP 4 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 2.11 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 4 completed successfully
Running apxGP 5 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 3.76 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 5 completed successfully
Running apxGP 6 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 4.34 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 6 completed successfully
Running apxGP 7 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 6.85 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 7 completed successfully
Running apxGP 8 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 4.58 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 8 completed successfully
Running apxGP 9 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 3.49 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 9 completed successfully
Running apxGP 10 with agents: 121
pxADMM optimizer initialized with rho: 0.85, lip: 1.0, tol_abs: 0.0001, tol_rel: 0.01
pxADMM converged at iteration 7
Rank 0: Convergence achieved at epoch 7 with loss: -0.2714729309082031
Rank 0: Training completed in 1.86 seconds.
[92mRank 0 - Testing RMSE: 3.2018[0m
[92mRank: 0, Lengthscale: [[0.67518896 0.6108726 ]] [0m
[92mRank: 0, Outputscale: 0.910112202167511 [0m
[92mRank: 0, Noise: 0.06287146359682083 [0m
Run 10 completed successfully
Running cGP 1 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008

Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371

Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
[92mRank 0 - Testing RMSE: 5.4695[0m
[92mRank: 0, Lengthscale: Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
[[0.12766229 0.11803288]] [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015

Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095

Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834

Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795

Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125

Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207

Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796

Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151

Run 1 completed successfully
Running cGP 2 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763

Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562


Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817

Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188

Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175

Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
[92mRank 0 - Testing RMSE: 5.4695[0mRank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684

Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656



Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581

Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612

Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389


[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]]Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
 [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Run 2 completed successfully
Running cGP 3 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763

Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649

Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282

Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962

Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326

Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
[92mRank 0 - Testing RMSE: 5.4695[0m
[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]] [0m
Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741[92mRank: 0, Outputscale: 0.23574402928352356 [0m

[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534


Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764

Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127

Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248

Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958

Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624


Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795

Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656

Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688

Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644

Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Run 3 completed successfully
Running cGP 4 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649

Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326

Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845

Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962

Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121

Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875

Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015

Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
[92mRank 0 - Testing RMSE: 5.4695[0m
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
[92mRank: 0, Lengthscale: Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
[[0.12766229 0.11803288]] [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795

Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796


Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837

Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791


Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724

Run 4 completed successfully
Running cGP 5 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625


Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649

Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675

Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565

Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788

Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151

Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
[92mRank 0 - Testing RMSE: 5.4695[0m
[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]] [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121

Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562

Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578

Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883

Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826

Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354

Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709

Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394

Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794

Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997


Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Run 5 completed successfully
Running cGP 6 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845

Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326
Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532

Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895

Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878

Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915


Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008

Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764


Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341

Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015

Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078

Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834


Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203


Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937

Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
[92mRank 0 - Testing RMSE: 5.4695[0mRank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207

[92mRank: 0, Lengthscale:Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
 [[0.12766229 0.11803288]]Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
 [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Run 6 completed successfully
Running cGP 7 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375

Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798


Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008


Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015

Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657

Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578

Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341


Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937

Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725

Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816

Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207

Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791


Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115

Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
[92mRank 0 - Testing RMSE: 5.4695[0mRank 9 - Training converged at epoch 137 with loss: 0.8795036673545837

[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]]Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
 [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Run 7 completed successfully
Running cGP 8 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845
Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969


Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878

Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534

Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025

Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464

Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879

Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138

Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624

Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684

Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186

Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
[92mRank 0 - Testing RMSE: 5.4695[0m
[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]] [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747

Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796

Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644


Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Run 8 completed successfully
Running cGP 9 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068
Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375

Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845


Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562
Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188
Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015
[92mRank 0 - Testing RMSE: 5.4695[0m
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
[92mRank: 0, Lengthscale: Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248

Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
[[0.12766229 0.11803288]]Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818

Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
 Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578[0m

[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395
Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747

Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581

Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483
Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Run 9 completed successfully
Running cGP 10 with agents: 121
cADMM optimizer initialized with rho: 0.85, max_iter: 5, lr: 0.005
Epoch 10/1000 Loss: 0.7883021831512451
Epoch 20/1000 Loss: 0.6795837879180908
Epoch 30/1000 Loss: 0.5725224614143372
Epoch 40/1000 Loss: 0.47535112500190735
Epoch 50/1000 Loss: 0.3968537747859955
Epoch 60/1000 Loss: 0.3423665463924408
Epoch 70/1000 Loss: 0.3128345310688019
Epoch 80/1000 Loss: 0.30602556467056274
Epoch 90/1000 Loss: 0.31607240438461304
Epoch 100/1000 Loss: 0.3313450813293457
Epoch 110/1000 Loss: 0.3349545896053314
Epoch 120/1000 Loss: 0.31344592571258545
Epoch 130/1000 Loss: 0.26423919200897217
Rank 113 - Training converged at epoch 137 with loss: 0.12008268386125565Rank 117 - Training converged at epoch 137 with loss: 0.3307962715625763
Rank 118 - Training converged at epoch 137 with loss: 0.2080620676279068Rank 119 - Training converged at epoch 137 with loss: 0.05060471594333649

Rank 111 - Training converged at epoch 137 with loss: 0.5988723039627075Rank 116 - Training converged at epoch 137 with loss: 0.37252649664878845

Rank 115 - Training converged at epoch 137 with loss: 0.31720149517059326
Rank 114 - Training converged at epoch 137 with loss: 0.12608793377876282
Rank 109 - Training converged at epoch 137 with loss: 0.09979629516601562Rank 112 - Training converged at epoch 137 with loss: 0.4325000047683716
Rank 105 - Training converged at epoch 137 with loss: 0.1531085968017578


Rank 120 - Training converged at epoch 137 with loss: -0.15816497802734375Rank 110 - Training converged at epoch 137 with loss: 0.4606674909591675

Rank 106 - Training converged at epoch 137 with loss: 0.07652010768651962
Rank 104 - Training converged at epoch 137 with loss: 0.12682300806045532
Rank 100 - Training converged at epoch 137 with loss: 0.7559047341346741Rank 96 - Training converged at epoch 137 with loss: 0.0545925572514534
Rank 99 - Training converged at epoch 137 with loss: 0.619181752204895Rank 92 - Training converged at epoch 137 with loss: 0.16165044903755188

Rank 90 - Training converged at epoch 137 with loss: 0.7193430662155151

Rank 107 - Training converged at epoch 137 with loss: 0.09743033349514008
Rank 88 - Training converged at epoch 137 with loss: 0.6987643837928772Rank 87 - Training converged at epoch 137 with loss: 0.41795969009399414
Rank 84 - Training converged at epoch 137 with loss: -0.29369208216667175
Rank 86 - Training converged at epoch 137 with loss: 0.4117293357849121

Rank 85 - Training converged at epoch 137 with loss: 0.11999008804559708
Rank 82 - Training converged at epoch 137 with loss: -0.096184641122818
Rank 80 - Training converged at epoch 137 with loss: 0.6994253396987915
Rank 79 - Training converged at epoch 137 with loss: 0.7926523089408875
Rank 95 - Training converged at epoch 137 with loss: -0.32175105810165405
Rank 76 - Training converged at epoch 137 with loss: 0.462395042181015
Rank 74 - Training converged at epoch 137 with loss: 0.13732825219631195
Rank 73 - Training converged at epoch 137 with loss: -0.10574323683977127
Rank 83 - Training converged at epoch 137 with loss: -0.46131718158721924
Rank 101 - Training converged at epoch 137 with loss: 0.6367164254188538
Rank 69 - Training converged at epoch 137 with loss: 0.7138810157775879
Rank 89 - Training converged at epoch 137 with loss: 0.8158137202262878
Rank 94 - Training converged at epoch 137 with loss: -0.21008522808551788
Rank 75 - Training converged at epoch 137 with loss: 0.3708273470401764
Rank 71 - Training converged at epoch 137 with loss: -0.14697647094726562
Rank 68 - Training converged at epoch 137 with loss: 0.7849019169807434
Rank 108 - Training converged at epoch 137 with loss: 0.22256523370742798
Rank 72 - Training converged at epoch 137 with loss: -0.4459613263607025
Rank 67 - Training converged at epoch 137 with loss: 0.9438678026199341
Rank 78 - Training converged at epoch 137 with loss: 0.9092301726341248
Rank 0 - Training converged at epoch 137 with loss: 0.2166900634765625
Rank 77 - Training converged at epoch 137 with loss: 0.839125394821167
Rank 70 - Training converged at epoch 137 with loss: 0.3802393674850464
Rank 64 - Training converged at epoch 137 with loss: 0.3688986003398895
Rank 65 - Training converged at epoch 137 with loss: 0.4907984137535095
Rank 103 - Training converged at epoch 137 with loss: 0.05213779956102371
Rank 102 - Training converged at epoch 137 with loss: 0.3383212685585022
Rank 97 - Training converged at epoch 137 with loss: 0.3459779918193817
Rank 81 - Training converged at epoch 137 with loss: 0.3524168133735657
Rank 62 - Training converged at epoch 137 with loss: 0.11523154377937317
Rank 63 - Training converged at epoch 137 with loss: 0.2110627442598343
Rank 60 - Training converged at epoch 137 with loss: -0.21413898468017578
Rank 57 - Training converged at epoch 137 with loss: 0.7863771915435791
Rank 93 - Training converged at epoch 137 with loss: -0.07581901550292969
Rank 61 - Training converged at epoch 137 with loss: -0.1486699879169464
Rank 54 - Training converged at epoch 137 with loss: 0.44331327080726624Rank 59 - Training converged at epoch 137 with loss: 0.305688738822937

Rank 49 - Training converged at epoch 137 with loss: -0.1382957100868225
Rank 46 - Training converged at epoch 137 with loss: 0.7315104603767395Rank 48 - Training converged at epoch 137 with loss: 0.10529538989067078

Rank 47 - Training converged at epoch 137 with loss: 0.49726712703704834
Rank 51 - Training converged at epoch 137 with loss: 0.37724852561950684
Rank 52 - Training converged at epoch 137 with loss: 0.34975019097328186
Rank 42 - Training converged at epoch 137 with loss: 0.496938019990921Rank 40 - Training converged at epoch 137 with loss: 0.6473151445388794
Rank 39 - Training converged at epoch 137 with loss: 0.5040305852890015
Rank 43 - Training converged at epoch 137 with loss: 0.44784724712371826
Rank 41 - Training converged at epoch 137 with loss: 0.5555323362350464
Rank 38 - Training converged at epoch 137 with loss: 0.2240467071533203
Rank 53 - Training converged at epoch 137 with loss: 0.38544973731040955
Rank 36 - Training converged at epoch 137 with loss: 0.34671536087989807
Rank 91 - Training converged at epoch 137 with loss: 0.5318996906280518
Rank 58 - Training converged at epoch 137 with loss: 0.6525691151618958
Rank 35 - Training converged at epoch 137 with loss: 0.6909730434417725
Rank 33 - Training converged at epoch 137 with loss: 0.5546686053276062
Rank 32 - Training converged at epoch 137 with loss: 0.5544412136077881
Rank 55 - Training converged at epoch 137 with loss: 0.8727843165397644
Rank 34 - Training converged at epoch 137 with loss: 0.8045299053192139
Rank 56 - Training converged at epoch 137 with loss: 0.949919581413269
Rank 29 - Training converged at epoch 137 with loss: 0.7980103492736816
Rank 45 - Training converged at epoch 137 with loss: 0.8704388737678528
Rank 37 - Training converged at epoch 137 with loss: 0.06321190297603607
Rank 27 - Training converged at epoch 137 with loss: 0.5466005802154541
Rank 25 - Training converged at epoch 137 with loss: 0.30185624957084656
Rank 26 - Training converged at epoch 137 with loss: 0.3378816246986389
Rank 31 - Training converged at epoch 137 with loss: 0.6612519025802612
Rank 20 - Training converged at epoch 137 with loss: 0.8577268123626709
Rank 22 - Training converged at epoch 137 with loss: 0.4309833347797394
Rank 24 - Training converged at epoch 137 with loss: 0.5893529057502747
Rank 23 - Training converged at epoch 137 with loss: 0.6840956807136536
Rank 21 - Training converged at epoch 137 with loss: 0.7136614322662354
Rank 19 - Training converged at epoch 137 with loss: 0.8783314824104309
Rank 30 - Training converged at epoch 137 with loss: 0.7021167278289795Rank 28 - Training converged at epoch 137 with loss: 0.7039573192596436


Rank 18 - Training converged at epoch 137 with loss: 0.9420989751815796
Rank 17 - Training converged at epoch 137 with loss: 0.8394699096679688
Rank 15 - Training converged at epoch 137 with loss: 0.5427443385124207
Rank 16 - Training converged at epoch 137 with loss: 0.68682861328125
Rank 13 - Training converged at epoch 137 with loss: 0.49304983019828796
Rank 14 - Training converged at epoch 137 with loss: 0.41892144083976746
Rank 12 - Training converged at epoch 137 with loss: 0.5748118162155151
Rank 9 - Training converged at epoch 137 with loss: 0.8795036673545837
Rank 8 - Training converged at epoch 137 with loss: 0.9312746524810791
Rank 50 - Training converged at epoch 137 with loss: 0.18853558599948883Rank 7 - Training converged at epoch 137 with loss: 0.9535101056098938
Rank 44 - Training converged at epoch 137 with loss: 0.7210583090782166
Rank 6 - Training converged at epoch 137 with loss: 0.816034734249115
Rank 5 - Training converged at epoch 137 with loss: 0.5987610220909119
Rank 4 - Training converged at epoch 137 with loss: 0.5014058351516724
Rank 98 - Training converged at epoch 137 with loss: 0.2906610667705536
Rank 10 - Training converged at epoch 137 with loss: 0.7184234261512756
Rank 2 - Training converged at epoch 137 with loss: 0.3232257664203644
Rank 1 - Training converged at epoch 137 with loss: 0.37442153692245483

Rank 3 - Training converged at epoch 137 with loss: 0.4085797965526581Rank 11 - Training converged at epoch 137 with loss: 0.3821060359477997

[92mRank 0 - Testing RMSE: 5.4695[0m
[92mRank: 0, Lengthscale: [[0.12766229 0.11803288]] [0m
[92mRank: 0, Outputscale: 0.23574402928352356 [0m
[92mRank: 0, Noise: 0.015174743719398975 [0m
Rank 66 - Training converged at epoch 137 with loss: 0.8898650407791138
Run 10 completed successfully
